import json
import os
import re
import shutil
from typing import Dict, List, Optional

import numpy as np
from openai import OpenAI
from agent.base_agent import BaseAgent, Controller
from agent.llm_hcg.knowledge_base import KnowledgeBase
from core.task import TaskRepresentation
from core.utils import get_error_info
from env.base_meta_env import BaseMetaEnv, Observation, ActionType, InfoDict
from abc import ABC, abstractmethod
import enum
import random
from typing import Any, Dict, Tuple, Union
from hydra.utils import instantiate
from llm import llm_name_to_LLMClass


class LLMBasedHierarchicalControllerGenerator(BaseAgent):

    def __init__(self, config: Dict):
        super().__init__(config)
        # Initialize OpenAI API
        name_llm = config["llm"]["name"]
        config_llm = config["llm"]["config"]
        self.llm = llm_name_to_LLMClass[name_llm](config_llm)

        # Extract the configuration parameters
        self.model = config["model"]
        self.num_attempts_sc = config.get("num_attempts_sc", 5)
        # Initialize agent's variables
        self.t = 0  # Time step
        # Define the base namespace that can be used by the assistant
        self.global_namespace = {}
        exec(open("agent/llm_hcg/base_namespace.py").read(), self.global_namespace)
        self.sc_code_last: Optional[str] = (
            None  # The code of the controller generated by the assistant
        )
        # Initialize knowledge base
        self.knowledge_base = KnowledgeBase(
            config_agent=config, namespace=self.global_namespace
        )

    def give_textual_description(self, description: str):
        self.description_env = description
        self.text_controller_base_class = open("agent/base_controller.py").read()
        self.text_agent_answer_example = open("assets/agent_answer_example.txt").read()

    def get_controller(self, task: TaskRepresentation) -> Controller:

        # Breakpoint-pause at each task if the debug mode is activated
        if self.config["config_debug"]["breakpoint_on_new_t"]:
            print(f"Task {self.t} : {task}. Press 'c' to continue.")
            breakpoint()

        # Create the prompt for the assistant
        prompt = (
            "You will be asked to generate the code for a Controller for a given task in a RL-like environment. "
            "The general description of the environment is the following:"
            "\n\n"
            "[--- General description of the environment ---]\n"
            f"{self.description_env}\n"
            "[--- End of the general description of the environment ---]"
            "\n\n"
            "A controller obeys the following interface:\n"
            "```python\n"
            f"{self.text_controller_base_class}"
            "```"
            "\n\n"
            "This can be a very hard task at the beginning since you have very little information about the environment and it's structure. "
            f"That is why you have at your disposal the following ressources :\n{self.knowledge_base}"
            "\n\n"
            "You should try as much as possible to produce controllers that are short in terms of tokens of code. "
            "This can be done in particular by re-using the functions and controllers that are already implemented in the knowledge base and won't cost a lot of tokens. "
            "\n\n"

            "Please reason step-by-step and think about the best way to solve the task before answering. "
            "Globally, your answer should be returned following that example:\n"
            "[--- Example of answer ---]\n"
            f"{self.text_agent_answer_example}"
            "[--- End of example of answer ---]"
            "\n\n"
            f"You will have to implement a controller (under the variable 'controller') to solve the following task : {task}."
        )

        # Iterate until the controller is generated. If error, log it in the message and ask the assistant to try again.
        is_controller_instance_generated = False
        name_to_text: Dict[str, str] = {}
        self.llm.reset()
        self.llm.add_prompt(prompt)
        for no_attempt in range(self.num_attempts_sc):
            # Ask the assistant
            answer = self.llm.generate()
            # Extract the code block from the answer
            sc_code = self.extract_SC_code(answer)
            if sc_code is None:
                # Retry if the code could not be extracted
                print(
                    f"WARNING : Could not extract the code from the answer. Asking the assistant to try again. (Attempt {no_attempt+1}/{self.num_attempts_sc})"
                )
                self.llm.add_answer(answer)
                self.llm.add_prompt(
                    "I'm sorry, extracting the code from your answer failed. Please try again and make sure the code obeys the following format:\n```python\n<your code here>\n```"
                )
                name_to_text[
                    f"assistant_answer_failed_{no_attempt}_extraction_reason.txt"
                ] = answer
                if self.config["config_debug"]["breakpoint_on_failed_sc_extraction"]:
                    print("sc_code extraction failed. Press 'c' to continue.")
                    breakpoint()
                continue
            # Execute the controller code and retrieve the controller instance
            try:
                self.exec_import_in_global_namespace(sc_code)
                controller_instance = self.get_controller_instance(sc_code)
            except Exception as e:
                full_error_info = get_error_info(e)
                print(
                    f"WARNING : Could not execute the code from the answer. Asking the assistant to try again (Attempt {no_attempt+1}/{self.num_attempts_sc}). Full error info : {full_error_info}"
                )
                self.llm.add_answer(answer)
                self.llm.add_prompt(
                    f"I'm sorry, an error occured while executing your code. Please try again and make sure the code is correct. Full error info : {full_error_info}"
                )
                name_to_text[
                    f"assistant_answer_failed_{no_attempt}_exec_reason.txt"
                ] = answer
                if self.config["config_debug"]["breakpoint_on_failed_sc_extraction"]:
                    input("Continue ? ...")
                continue
            # Save the code for update step and return the controller instance
            self.sc_code_last = sc_code
            is_controller_instance_generated = True
            break

        if is_controller_instance_generated:
            config_logs = self.config["config_logs"]
            log_dir = config_logs["log_dir"]
            list_run_names = []
            if config_logs["do_log_on_new"]:
                list_run_names.append(self.config["run_name"])
            if config_logs["do_log_on_last"]:
                list_run_names.append("_last")
            for run_name in list_run_names:
                path_task_t = os.path.join(log_dir, run_name, f"task_{self.t}")
                name_to_text.update(
                    {
                        "config.yaml": json.dumps(self.config, indent=4),
                        "prompt.txt": prompt,
                        "assistant_answer.txt": answer,
                        "controller.py": sc_code,
                    }
                )
                self.log_texts(
                    log_dir=path_task_t,
                    name_to_text=name_to_text,
                )
            return controller_instance

        else:
            raise ValueError(
                f"Could not generate a controller after {self.num_attempts_sc} attempts. Stopping the process."
            )

    def update(
        self,
        task: TaskRepresentation,
        controller: Controller,
        feedback: Dict[str, Union[float, str]],
    ):
        self.t += 1

    # ================ Helper functions ================

    def extract_SC_code(self, answer: str) -> str:
        """Extracts the controller definition and instantiation code from an LLM response.
        The answer should contain one python code block with the controller code and instanciate a Controller variable named 'controller'.

        Args:
            answer (str): the answer from the LLM.

        Returns:
            str: the controller code.
        """
        sc_match = re.search(r"```python\n(.*?)\n```", answer, re.DOTALL)
        sc_code = sc_match.group(1).strip() if sc_match else None
        return sc_code

    def exec_import_in_global_namespace(self, code: str):
        """From a code structured as [imports, other], execute the import statements in the global namespace.
        This method is usefull to add the imports in the global namespace so that they can be used in the code,
        without having to execute the whole code in the global namespace as this may cause conflicts.

        Args:
            code (str): a string containing the code.
            This code should be composed of :
                - import statements
                - other code
        """
        import_pattern = re.compile(
            r"^(?:from\s+\S+\s+import\s+\S+|import\s+\S+.*)$", re.MULTILINE
        )
        import_matches = import_pattern.findall(code)
        import_code = "\n".join(import_matches)
        exec(import_code, self.global_namespace)

    def get_controller_instance(self, code: str) -> Controller:
        """From a code structured as [imports, class definition, controller instantiation], extract the controller instance.

        Args:
            code (str): a string containing the code.
            This code should be composed of :
                - import statements
                - a class definition
                - the instanciation of a controller object named 'controller'

        Returns:
            Controller: the controller instance.
        """
        temp_namespace = {}
        exec(code, self.global_namespace, temp_namespace)
        assert (
            "controller" in temp_namespace
        ), "The controller variable was not defined in the code."
        return temp_namespace.get("controller")

    def log_texts(
        self,
        log_dir: str,
        name_to_text: Dict[str, str],
    ):
        """Log texts in a directory. For each (key, value) in the directory, the file "dir_log/key" will contain the value.
        Remove the directory if it already exists.

        Args:
            log_dir (str): _description_
            name_to_text (Dict[str, str]): a dictionnary containing the name of the file to create and the text to write in it.
        """
        os.makedirs(log_dir, exist_ok=True)
        for name, text in name_to_text.items():
            with open(os.path.join(log_dir, name), "w") as f:
                f.write(text)
